{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Classification using PreResNet110\n",
    "## Comparing results of SGD and SGD with landscape modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "iMN3QVUOSmVq",
    "outputId": "94e5fdff-77fb-4eb8-8c7e-3492a02d22e3"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# import modified optimizer\n",
    "from SGD_IKSA import SGD_IKSA\n",
    "# import model\n",
    "from preresnet import PreResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the seed at the beginning of each experiment to ensure reproducibility.\n",
    "# We also make sure no non-deterministic methods are used.\n",
    "def setup_seed(s):\n",
    "    torch.manual_seed(s)\n",
    "    random.seed(s)  \n",
    "    numpy.random.seed(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that will perform an experiment, by training PreResNet on CIFAR10, using both SGD and SGD with landscape modification, and report the results.\n",
    "\n",
    "Training process inspired by: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html, https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5TaGYuqcSvWH"
   },
   "outputs": [],
   "source": [
    "def compare_SGD_SGD_LM(device, \n",
    "                       seed, \n",
    "                       model, \n",
    "                       trainset, \n",
    "                       testset, \n",
    "                       batch_size, \n",
    "                       no_epochs, \n",
    "                       lr, \n",
    "                       momentum, \n",
    "                       weight_decay, \n",
    "                       optimizer_type, \n",
    "                       LM_f, \n",
    "                       LM_c, \n",
    "                       LM_c_run_min):\n",
    "    \"\"\"This function trains a given model on a given train dataset, using both SGD \n",
    "    and SGD with landscape modification. Subsequently, it computes the testing accuracy on the given test data.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        device : CPU/GPU used for training process;\n",
    "        seed : seed to be used in the random processes involved;\n",
    "        model : classification model to be used;\n",
    "        trainset : training data;\n",
    "        testset : testing data;\n",
    "        batch_size: batch-size to be used in training;\n",
    "        no_epochs : number of epochs that the model will be trained for;\n",
    "        lr : learninf rate of SGD optimizer;\n",
    "        momentum : momentum value of the SGD optimizer;\n",
    "        weight_decay : weight decay value of the SGD optimizer;\n",
    "        optimizer_type : bool indicating whether landscape modification will be used\n",
    "        LM_f : function to be used when performing landscape modification\n",
    "        LM_c : c value to be used when performing landscape modification\n",
    "        LM_c_run_min : bool indicating whether c will be be taken as the running minimum of \n",
    "                       U(x), with LM_c as initial value.\n",
    "\n",
    "    Returns:\n",
    "        [dict]: a dictionary that contains: a dictionary of parameters, a list of loss values,\n",
    "        a list of c values, a list of average loss values, a list of testing accuracy values,\n",
    "        and the final testing accuracy value.\n",
    "    \"\"\"\n",
    "    # We want to save the hyperparameters used for this experiment in a dictionary.\n",
    "    param_dict = {\"no_epochs\": no_epochs,\n",
    "                  \"batch_size\": batch_size,\n",
    "                  \"lr\": lr,\n",
    "                  \"momentum\": momentum,\n",
    "                  \"wd\": weight_decay,\n",
    "                  \"optimizer_type\": optimizer_type,\n",
    "                  \"LM_f\": LM_f.__name__,\n",
    "                  \"LM_c\": LM_c,\n",
    "                  \"LM_c_run_min\": LM_c_run_min}\n",
    "    \n",
    "    # We keep the loss values.\n",
    "    loss_list = []\n",
    "    # We keep the c values.\n",
    "    c_list = []\n",
    "    # We save a list of loss values averaged every 20 minibatches.\n",
    "    average_loss_list = []\n",
    "    # We save a list of accuracy scores for each epoch.\n",
    "    accuracy_list = []\n",
    "\n",
    "    # We set up a seed.\n",
    "    setup_seed(seed)\n",
    "\n",
    "    # We preserve reproducibility in the data loading process\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    # We prepare our data for training.\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=4, worker_init_fn = seed_worker, generator = g)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=4, worker_init_fn = seed_worker, generator = g)\n",
    "\n",
    "    # Move model to device.\n",
    "    model.to(device)\n",
    "\n",
    "    # We define the loss function.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # We define an optimizer, depending on the optimizer type given.\n",
    "    if optimizer_type == \"Original\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_type == \"LM\":\n",
    "        optimizer = SGD_IKSA(model.parameters(), LM_f, lr=lr, momentum=momentum, weight_decay= weight_decay)\n",
    "        \n",
    "    # We train the model for the given number of epochs.\n",
    "    for epoch in range(no_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        # We go through all minibatches in the trainloader:\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            \n",
    "            # We move data to device.\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            # We empty the gradients of the parameters.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # We compute predictions.\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # We compute the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss)\n",
    "            # We backpropagate.\n",
    "            loss.backward()\n",
    "\n",
    "            # If we are using landscape modification, check if we are taking \n",
    "            # the running loss as c.\n",
    "            if optimizer_type == \"LM\":\n",
    "                if LM_c_run_min:\n",
    "                    # Update c as the running minimum of the loss.\n",
    "                    if loss < LM_c:\n",
    "                      LM_c = loss.item()\n",
    "                # Let the optimizer take a step.\n",
    "                optimizer.step(LM_c, loss)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            c_list.append(LM_c)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Compute the average loss every 20 mini-batches.\n",
    "            if i % 20 == 19:\n",
    "                print(epoch + 1, i + 1, running_loss / 20)\n",
    "                average_loss_list.append(running_loss / 20)\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # For each epoch, compute testing accuracy.\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            # Iterate through the testing data.\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Compute output.\n",
    "                outputs = model(images)\n",
    "                # Compute prediction, by taking the max output.\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        # Compute accuracy.\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('Accuracy:', (100 * correct / total))\n",
    "\n",
    "\n",
    "    print('End of training.')\n",
    "\n",
    "    # Compute final testing accuracy:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        # Iterate through the testing data.\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Compute output.\n",
    "            outputs = model(images)\n",
    "            # Compute prediction, by taking the max output.\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    # Compute accuracy.\n",
    "    accuracy = 100 * correct / total\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('Accuracy:', (100 * correct / total))\n",
    "\n",
    "    # Return a dictionary with all necessary information.\n",
    "    results_dict = {\"param_dict\": param_dict,\n",
    "                    \"loss_list\": loss_list,\n",
    "                    \"c_list\": c_list,\n",
    "                    \"average_loss_list\": average_loss_list,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"accuracy_list\": accuracy_list}\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some functions that create useful reports for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_accuracy_list(list_of_results_dicts):\n",
    "    \"\"\"This function takes a list of dictionaries as resulted from \n",
    "    the function defined above, and produces a dataframe with \n",
    "    accuracy values for the two models (SGD with/without landscape modification).\n",
    "    \"\"\"\n",
    "    accuracy_dict = {}\n",
    "    # Iterate through all dictionaries of results:\n",
    "    for d in list_of_results_dicts:\n",
    "        optimizer = d[\"param_dict\"][\"optimizer_type\"]\n",
    "        epochs = d[\"param_dict\"][\"no_epochs\"]\n",
    "        lr = d[\"param_dict\"][\"lr\"]\n",
    "        momentum = d[\"param_dict\"][\"momentum\"]\n",
    "        wd = d[\"param_dict\"][\"wd\"]\n",
    "        f = d[\"param_dict\"][\"LM_f\"] # will need to be formatted to string\n",
    "        c = d[\"param_dict\"][\"LM_c\"]\n",
    "        c_running_loss = str(d[\"param_dict\"][\"LM_c_run_min\"])\n",
    "        \n",
    "        # We create a unique key for each experiment.\n",
    "        key = f\"{optimizer}_{epochs}_{lr}_{momentum}_{wd}_{f}_{c}_{c_running_loss}\"\n",
    "        # We assign the corresponding loss values to the key.\n",
    "        value = d[\"accuracy_list\"]\n",
    "        # We create a dictionary that will be turned to a dataframe.\n",
    "        accuracy_dict[key] = value\n",
    "\n",
    "    accuracy_df = pd.DataFrame(accuracy_dict)\n",
    "\n",
    "    return accuracy_df\n",
    "\n",
    "\n",
    "def create_report(list_of_results_dicts):\n",
    "\n",
    "    columns = [\"Optimizer\", \"Epochs\", \"Learning rate\", \"Momentum\", \"Weight Decay\", \"LM_f\", \"LM_C\", \"C_running_min\",\\\n",
    "                \"Min Loss\", \"Last Loss\", \"Last Average Loss\", \"Test Accuracy\"]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for d in list_of_results_dicts:\n",
    "\n",
    "        optimizer = d[\"param_dict\"][\"optimizer_type\"]\n",
    "        epochs = d[\"param_dict\"][\"no_epochs\"]\n",
    "        lr = d[\"param_dict\"][\"lr\"]\n",
    "        momentum = d[\"param_dict\"][\"momentum\"]\n",
    "        wd = d[\"param_dict\"][\"wd\"]\n",
    "        f = d[\"param_dict\"][\"LM_f\"] # will need to be formatted to string\n",
    "        c = d[\"param_dict\"][\"LM_c\"]\n",
    "        c_running_loss = str(d[\"param_dict\"][\"LM_c_run_min\"])\n",
    "        min_loss = min(d[\"loss_list\"]).item()\n",
    "        last_loss = d[\"loss_list\"][-1].item()\n",
    "        last_average_loss = d[\"average_loss_list\"][-1]\n",
    "        test_accuracy = d[\"accuracy\"]\n",
    "        rows.append([optimizer, epochs, lr, momentum, wd,\\\n",
    "            f, c, c_running_loss, min_loss, last_loss, last_average_loss, test_accuracy])\n",
    "\n",
    "\n",
    "    report_df = pd.DataFrame(columns = columns, data = rows)\n",
    "\n",
    "    return report_df\n",
    "\n",
    "def create_loss_sheet(list_of_results_dicts):\n",
    "\n",
    "    loss_dict = {}\n",
    "    for d in list_of_results_dicts:\n",
    "\n",
    "        optimizer = d[\"param_dict\"][\"optimizer_type\"]\n",
    "        epochs = d[\"param_dict\"][\"no_epochs\"]\n",
    "        lr = d[\"param_dict\"][\"lr\"]\n",
    "        momentum = d[\"param_dict\"][\"momentum\"]\n",
    "        wd = d[\"param_dict\"][\"wd\"]\n",
    "        f = d[\"param_dict\"][\"LM_f\"] # will need to be formatted to string\n",
    "        c = d[\"param_dict\"][\"LM_c\"]\n",
    "        c_running_loss = str(d[\"param_dict\"][\"LM_c_run_min\"])\n",
    "\n",
    "        key = f\"{optimizer}_{epochs}_{lr}_{momentum}_{wd}_{f}_{c}_{c_running_loss}\"\n",
    "        value = d[\"loss_list\"]\n",
    "\n",
    "        # turn into list of floats\n",
    "        map_obj = map(torch.Tensor.item, value)\n",
    "        value = list(map_obj)\n",
    "\n",
    "        loss_dict[key] = value\n",
    "\n",
    "    loss_df = pd.DataFrame(loss_dict)\n",
    "\n",
    "    return loss_df\n",
    "\n",
    "def create_c_sheet(list_of_results_dicts):\n",
    "\n",
    "    c_dict = {}\n",
    "    for d in list_of_results_dicts:\n",
    "\n",
    "        optimizer = d[\"param_dict\"][\"optimizer_type\"]\n",
    "        epochs = d[\"param_dict\"][\"no_epochs\"]\n",
    "        lr = d[\"param_dict\"][\"lr\"]\n",
    "        momentum = d[\"param_dict\"][\"momentum\"]\n",
    "        wd = d[\"param_dict\"][\"wd\"]\n",
    "        f = d[\"param_dict\"][\"LM_f\"] # will need to be formatted to string\n",
    "        c = d[\"param_dict\"][\"LM_c\"]\n",
    "        c_running_loss = str(d[\"param_dict\"][\"LM_c_run_min\"])\n",
    "\n",
    "        key = f\"{optimizer}_{epochs}_{lr}_{momentum}_{wd}_{f}_{c}_{c_running_loss}\"\n",
    "        value = d[\"c_list\"]\n",
    "        \n",
    "        c_dict[key] = value\n",
    "\n",
    "    c_df = pd.DataFrame(c_dict)\n",
    "\n",
    "    return c_df\n",
    "\n",
    "\n",
    "def create_average_loss_sheet(list_of_results_dicts):\n",
    "\n",
    "    average_loss_dict = {}\n",
    "    for d in list_of_results_dicts:\n",
    "\n",
    "        optimizer = d[\"param_dict\"][\"optimizer_type\"]\n",
    "        epochs = d[\"param_dict\"][\"no_epochs\"]\n",
    "        lr = d[\"param_dict\"][\"lr\"]\n",
    "        momentum = d[\"param_dict\"][\"momentum\"]\n",
    "        wd = d[\"param_dict\"][\"wd\"]\n",
    "        f = d[\"param_dict\"][\"LM_f\"] # will need to be formatted to string\n",
    "        c = d[\"param_dict\"][\"LM_c\"]\n",
    "        c_running_loss = str(d[\"param_dict\"][\"LM_c_run_min\"])\n",
    "\n",
    "        key = f\"{optimizer}_{epochs}_{lr}_{momentum}_{wd}_{f}_{c}_{c_running_loss}\"\n",
    "        value = d[\"average_loss_list\"]\n",
    "\n",
    "        average_loss_dict[key] = value\n",
    "\n",
    "    average_loss_df = pd.DataFrame(average_loss_dict)\n",
    "\n",
    "    return average_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load our CIFAR10 data, applying transformations as per https://github.com/timgaripov/swa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8NdYqL6Tyd4",
    "outputId": "df886f7e-171f-469b-eaa3-4ee46ee6d9e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8cb4d7b10f4913b42f7bd7f01db485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "setup_seed(seed)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Qw-pONafc5J",
    "outputId": "e460d37b-6541-485a-e277-218c96a1a5df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a GPU if available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# SGD Parameters\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "wd = 0\n",
    "\n",
    "# LM parameters\n",
    "def x(x):\n",
    "    return x\n",
    "\n",
    "c = 10**5\n",
    "\n",
    "# seeds \n",
    "seed_list = [0, 10, 100]\n",
    "\n",
    "# number of epochs\n",
    "epochs = 150\n",
    "\n",
    "# model\n",
    "model = PreResNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AUjt_qNuf-Kn",
    "outputId": "a3e3a292-7cb2-4540-ff92-ef13c44df478"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'compare_SGD_SGD_LM.<locals>.seed_worker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zf/x6cvdkf552j1bvpgdkrfxmfh0000gn/T/ipykernel_51807/237461394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresults_dict_SGD\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcompare_SGD_SGD_LM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Original\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfinal_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dict_SGD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mresults_dict_SGD_LM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_SGD_SGD_LM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zf/x6cvdkf552j1bvpgdkrfxmfh0000gn/T/ipykernel_51807/3607641384.py\u001b[0m in \u001b[0;36mcompare_SGD_SGD_LM\u001b[0;34m(device, seed, model, trainset, testset, batch_size, no_epochs, lr, momentum, weight_decay, optimizer_type, LM_f, LM_c, LM_c_run_min)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# We go through all minibatches in the trainloader:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# We move data to device.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mset_spawning_popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mForkingPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'compare_SGD_SGD_LM.<locals>.seed_worker'"
     ]
    }
   ],
   "source": [
    "final_results = []\n",
    "\n",
    "for seed in seed_list:\n",
    "    results_dict_SGD =  compare_SGD_SGD_LM(device, seed, model, trainset, testset, batch_size, epochs, lr, momentum, wd, \"Original\", x, c, True)\n",
    "    final_results.append(results_dict_SGD)\n",
    "    results_dict_SGD_LM = compare_SGD_SGD_LM(device, seed, model, trainset, testset, batch_size, epochs, lr, momentum, wd, \"LM\", x, c, True)\n",
    "    final_results.append(results_dict_SGD_LM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aaD-UdSgZ1V"
   },
   "outputs": [],
   "source": [
    "report_df = create_report(final_results)\n",
    "report_df.to_csv(\"report.csv\")\n",
    "files.download(\"report.csv\")\n",
    "loss_df = create_loss_sheet(final_results)\n",
    "loss_df.to_csv(\"report1.csv\")\n",
    "files.download(\"report1.csv\")\n",
    "avg_loss_df = create_average_loss_sheet(final_results)\n",
    "avg_loss_df.to_csv(\"report2.csv\")\n",
    "files.download(\"report2.csv\")\n",
    "c_list = create_c_sheet(final_results)\n",
    "c_list.to_csv(\"report3.csv\")\n",
    "files.download(\"report3.csv\")\n",
    "accuracy_list = create_report_accuracy_list(final_results)\n",
    "accuracy_list.to_csv(\"report4.csv\")\n",
    "files.download(\"report4.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CIFAR10_SGD_test_functions",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
